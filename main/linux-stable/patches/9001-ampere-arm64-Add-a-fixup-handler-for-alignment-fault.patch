From 43154c8bc3ab56e5186ffaa669d3b2274c819ef4 Mon Sep 17 00:00:00 2001
From: D Scott Phillips <scott@os.amperecomputing.com>
Date: Tue, 13 Feb 2024 09:01:07 -0800
Subject: [PATCH 1/2] ampere/arm64: Add a fixup handler for alignment faults in
 aarch64 code

A later patch will hand out Device memory in some cases to code
which expects a Normal memory type, as an errata workaround.
Unaligned accesses to Device memory will fault though, so here we
add a fixup handler to emulate faulting accesses, at a performance
penalty.

Not all instructions in the Loads and Stores group are supported.
Unsupported instructions are:

 * Load/store memory tags
 * Load/store exclusive
 * LDAPR/STLR (unscaled immediate)
 * Load register (literal) [cannot Alignment fault]
 * Load/store register (unprivileged)
 * Atomic memory operations
 * Load/store register (pac)

Instruction implementations are translated from the Exploration tools'
ASL specifications.

Signed-off-by: D Scott Phillips <scott@os.amperecomputing.com>
---
 arch/arm64/include/asm/exception.h |    1 +
 arch/arm64/kernel/Makefile         |    2 +-
 arch/arm64/kernel/alignment.c      | 1049 ++++++++++++++++++++++++++++
 arch/arm64/mm/fault.c              |    5 +-
 4 files changed, 1054 insertions(+), 3 deletions(-)
 create mode 100644 arch/arm64/kernel/alignment.c

diff --git a/arch/arm64/include/asm/exception.h b/arch/arm64/include/asm/exception.h
index ad688e157c9be..ac87282f790b6 100644
--- a/arch/arm64/include/asm/exception.h
+++ b/arch/arm64/include/asm/exception.h
@@ -67,6 +67,7 @@ void do_el0_sys(unsigned long esr, struct pt_regs *regs);
 void do_sp_pc_abort(unsigned long addr, unsigned long esr, struct pt_regs *regs);
 void bad_el0_sync(struct pt_regs *regs, int reason, unsigned long esr);
 void do_el0_cp15(unsigned long esr, struct pt_regs *regs);
+int do_alignment_fixup(unsigned long addr, unsigned int esr, struct pt_regs *regs);
 int do_compat_alignment_fixup(unsigned long addr, struct pt_regs *regs);
 void do_el0_svc(struct pt_regs *regs);
 void do_el0_svc_compat(struct pt_regs *regs);
diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
index d95b3d6b471a7..7bb50f26a85a0 100644
--- a/arch/arm64/kernel/Makefile
+++ b/arch/arm64/kernel/Makefile
@@ -26,7 +26,7 @@ KCOV_INSTRUMENT_entry-common.o := n
 KCOV_INSTRUMENT_idle.o := n
 
 # Object file lists.
-obj-y			:= debug-monitors.o entry.o irq.o fpsimd.o		\
+obj-y			:= alignment.o debug-monitors.o entry.o irq.o fpsimd.o	\
 			   entry-common.o entry-fpsimd.o process.o ptrace.o	\
 			   setup.o signal.o sys.o stacktrace.o time.o traps.o	\
 			   io.o vdso.o hyp-stub.o psci.o cpu_ops.o		\
diff --git a/arch/arm64/kernel/alignment.c b/arch/arm64/kernel/alignment.c
new file mode 100644
index 0000000000000..dd5028398a4c8
--- /dev/null
+++ b/arch/arm64/kernel/alignment.c
@@ -0,0 +1,1049 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 Ampere Computing LLC
+ */
+
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/perf_event.h>
+#include <linux/printk.h>
+#include <linux/uaccess.h>
+
+#include <asm/exception.h>
+#include <asm/insn.h>
+#include <asm/patching.h>
+#include <asm/ptrace.h>
+#include <asm/traps.h>
+
+static __always_inline int __aarch64_insn_is_class_ldst(u32 insn)
+{
+	return (insn & 0x0A000000) == 0x08000000;
+}
+
+static __always_inline int __aarch64_insn_is_dc_zva(u32 insn)
+{
+	return (insn & 0xFFFFFFE0) == 0xD50B7420;
+}
+
+static int copy_from_user_io(void *to, const void __user *from, unsigned long n)
+{
+	const u8 __user *src = from;
+	u8 *dest = to;
+
+	for (; n; n--)
+		if (get_user(*dest++, src++))
+			break;
+	return n;
+}
+
+static int copy_to_user_io(void __user *to, const void *from, unsigned long n)
+{
+	const u8 *src = from;
+	u8 __user *dest = to;
+
+	for (; n; n--)
+		if (put_user(*src++, dest++))
+			break;
+	return n;
+}
+
+static int align_load(unsigned long addr, int sz, u64 *out)
+{
+	union {
+		u8 d8;
+		u16 d16;
+		u32 d32;
+		u64 d64;
+		char c[8];
+	} data;
+
+	if (sz != 1 && sz != 2 && sz != 4 && sz != 8)
+		return 1;
+	if (is_ttbr0_addr(addr)) {
+		if (copy_from_user_io(data.c, (const void __user *)addr, sz))
+			return 1;
+	} else
+		memcpy_fromio(data.c, (const void __iomem *)addr, sz);
+	switch (sz) {
+	case 1:
+		*out = data.d8;
+		break;
+	case 2:
+		*out = data.d16;
+		break;
+	case 4:
+		*out = data.d32;
+		break;
+	case 8:
+		*out = data.d64;
+		break;
+	default:
+		return 1;
+	}
+	return 0;
+}
+
+static int align_store(unsigned long addr, int sz, u64 val)
+{
+	union {
+		u8 d8;
+		u16 d16;
+		u32 d32;
+		u64 d64;
+		char c[8];
+	} data;
+
+	switch (sz) {
+	case 1:
+		data.d8 = val;
+		break;
+	case 2:
+		data.d16 = val;
+		break;
+	case 4:
+		data.d32 = val;
+		break;
+	case 8:
+		data.d64 = val;
+		break;
+	default:
+		return 1;
+	}
+	if (is_ttbr0_addr(addr)) {
+		if (copy_to_user_io((void __user *)addr, data.c, sz))
+			return 1;
+	} else
+		memcpy_toio((void __iomem *)addr, data.c, sz);
+	return 0;
+}
+
+static int align_dc_zva(unsigned long addr, struct pt_regs *regs)
+{
+	int bs = read_cpuid(DCZID_EL0) & 0xf;
+	int sz = 1 << (bs + 2);
+
+	addr &= ~(sz - 1);
+	if (is_ttbr0_addr(addr)) {
+		for (; sz; sz--) {
+			if (align_store(addr, 1, 0))
+				return 1;
+		}
+	} else
+		memset_io((void __iomem *)addr, 0, sz);
+	return 0;
+}
+
+static u64 get_vn_dt(int n, int t)
+{
+	u64 res;
+
+	switch (n) {
+#define V(n)						\
+	case n:						\
+		asm("cbnz %w1, 1f\n\t"			\
+		    "mov %0, v"#n".d[0]\n\t"		\
+		    "b 2f\n\t"				\
+		    "1: mov %0, v"#n".d[1]\n\t"		\
+		    "2:" : "=r" (res) : "r" (t));	\
+		break					\
+
+	V(0);  V(1);  V(2);  V(3);  V(4);  V(5);  V(6);  V(7);
+	V(8);  V(9);  V(10); V(11); V(12); V(13); V(14); V(15);
+	V(16); V(17); V(18); V(19); V(20); V(21); V(22); V(23);
+	V(24); V(25); V(26); V(27); V(28); V(29); V(30); V(31);
+#undef V
+	default:
+		res = 0;
+		break;
+	}
+	return res;
+}
+
+static void set_vn_dt(int n, int t, u64 val)
+{
+	switch (n) {
+#define V(n)						\
+	case n:						\
+		asm("cbnz %w1, 1f\n\t"			\
+		    "mov v"#n".d[0], %0\n\t"		\
+		    "b 2f\n\t"				\
+		    "1: mov v"#n".d[1], %0\n\t"		\
+		    "2:" :: "r" (val), "r" (t));	\
+		break					\
+
+	V(0);  V(1);  V(2);  V(3);  V(4);  V(5);  V(6);  V(7);
+	V(8);  V(9);  V(10); V(11); V(12); V(13); V(14); V(15);
+	V(16); V(17); V(18); V(19); V(20); V(21); V(22); V(23);
+	V(24); V(25); V(26); V(27); V(28); V(29); V(30); V(31);
+#undef Q
+	default:
+		break;
+	}
+}
+
+static u64 replicate64(u64 val, int bits)
+{
+	switch (bits) {
+	case 8:
+		val = (val << 8) | (val & 0xff);
+		fallthrough;
+	case 16:
+		val = (val << 16) | (val & 0xffff);
+		fallthrough;
+	case 32:
+		val = (val << 32) | (val & 0xffffffff);
+		break;
+	default:
+		break;
+	}
+	return val;
+}
+
+static u64 elem_get(u64 hi, u64 lo, int index, int esize)
+{
+	int shift = index * esize;
+	u64 mask = GENMASK(esize - 1, 0);
+
+	if (shift < 64)
+		return (lo >> shift) & mask;
+	else
+		return (hi >> (shift - 64)) & mask;
+}
+
+static void elem_set(u64 *hi, u64 *lo, int index, int esize, u64 val)
+{
+	int shift = index * esize;
+	u64 mask = GENMASK(esize - 1, 0);
+
+	if (shift < 64)
+		*lo = (*lo & ~(mask << shift)) | ((val & mask) << shift);
+	else
+		*hi = (*hi & ~(mask << (shift - 64))) | ((val & mask) << (shift - 64));
+}
+
+static int align_ldst_pair(u32 insn, struct pt_regs *regs)
+{
+	const u32 OPC = GENMASK(31, 30);
+	const u32 L_MASK = BIT(22);
+
+	int opc = FIELD_GET(OPC, insn);
+	int L = FIELD_GET(L_MASK, insn);
+
+	bool wback = !!(insn & BIT(23));
+	bool postindex = !(insn & BIT(24));
+
+	int n = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RN, insn);
+	int t = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT, insn);
+	int t2 = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT2, insn);
+	bool is_store = !L;
+	bool is_signed = !!(opc & 1);
+	int scale = 2 + (opc >> 1);
+	int datasize = 8 << scale;
+	u64 uoffset = aarch64_insn_decode_immediate(AARCH64_INSN_IMM_7, insn);
+	s64 offset = sign_extend64(uoffset, 6) << scale;
+	u64 address;
+	u64 data1, data2;
+	u64 dbytes;
+
+	if ((is_store && (opc & 1)) || opc == 3)
+		return 1;
+
+	if (wback && (t == n || t2 == n) && n != 31)
+		return 1;
+
+	if (!is_store && t == t2)
+		return 1;
+
+	dbytes = datasize / 8;
+
+	address = regs_get_register(regs, n << 3);
+
+	if (!postindex)
+		address += offset;
+
+	if (is_store) {
+		data1 = pt_regs_read_reg(regs, t);
+		data2 = pt_regs_read_reg(regs, t2);
+		if (align_store(address, dbytes, data1) ||
+		    align_store(address + dbytes, dbytes, data2))
+			return 1;
+	} else {
+		if (align_load(address, dbytes, &data1) ||
+		    align_load(address + dbytes, dbytes, &data2))
+			return 1;
+		if (is_signed) {
+			data1 = sign_extend64(data1, datasize - 1);
+			data2 = sign_extend64(data2, datasize - 1);
+		}
+		pt_regs_write_reg(regs, t, data1);
+		pt_regs_write_reg(regs, t2, data2);
+	}
+
+	if (wback) {
+		if (postindex)
+			address += offset;
+		if (n == 31)
+			regs->sp = address;
+		else
+			pt_regs_write_reg(regs, n, address);
+	}
+
+	return 0;
+}
+
+static int align_ldst_pair_simdfp(u32 insn, struct pt_regs *regs)
+{
+	const u32 OPC = GENMASK(31, 30);
+	const u32 L_MASK = BIT(22);
+
+	int opc = FIELD_GET(OPC, insn);
+	int L = FIELD_GET(L_MASK, insn);
+
+	bool wback = !!(insn & BIT(23));
+	bool postindex = !(insn & BIT(24));
+
+	int n = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RN, insn);
+	int t = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT, insn);
+	int t2 = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT2, insn);
+	bool is_store = !L;
+	int scale = 2 + opc;
+	int datasize = 8 << scale;
+	u64 uoffset = aarch64_insn_decode_immediate(AARCH64_INSN_IMM_7, insn);
+	s64 offset = sign_extend64(uoffset, 6) << scale;
+	u64 address;
+	u64 data1_d0, data1_d1, data2_d0, data2_d1;
+	u64 dbytes;
+
+	if (opc == 0x3)
+		return 1;
+
+	if (!is_store && t == t2)
+		return 1;
+
+	dbytes = datasize / 8;
+
+	address = regs_get_register(regs, n << 3);
+
+	if (!postindex)
+		address += offset;
+
+	if (is_store) {
+		data1_d0 = get_vn_dt(t, 0);
+		data2_d0 = get_vn_dt(t2, 0);
+		if (datasize == 128) {
+			data1_d1 = get_vn_dt(t, 1);
+			data2_d1 = get_vn_dt(t2, 1);
+			if (align_store(address, 8, data1_d0) ||
+			    align_store(address + 8, 8, data1_d1) ||
+			    align_store(address + 16, 8, data2_d0) ||
+			    align_store(address + 24, 8, data2_d1))
+				return 1;
+		} else {
+			if (align_store(address, dbytes, data1_d0) ||
+			    align_store(address + dbytes, dbytes, data2_d0))
+				return 1;
+		}
+	} else {
+		if (datasize == 128) {
+			if (align_load(address, 8, &data1_d0) ||
+			    align_load(address + 8, 8, &data1_d1) ||
+			    align_load(address + 16, 8, &data2_d0) ||
+			    align_load(address + 24, 8, &data2_d1))
+				return 1;
+		} else {
+			if (align_load(address, dbytes, &data1_d0) ||
+			    align_load(address + dbytes, dbytes, &data2_d0))
+				return 1;
+			data1_d1 = data2_d1 = 0;
+		}
+		set_vn_dt(t, 0, data1_d0);
+		set_vn_dt(t, 1, data1_d1);
+		set_vn_dt(t2, 0, data2_d0);
+		set_vn_dt(t2, 1, data2_d1);
+	}
+
+	if (wback) {
+		if (postindex)
+			address += offset;
+		if (n == 31)
+			regs->sp = address;
+		else
+			pt_regs_write_reg(regs, n, address);
+	}
+
+	return 0;
+}
+
+static int align_ldst_regoff(u32 insn, struct pt_regs *regs)
+{
+	const u32 SIZE = GENMASK(31, 30);
+	const u32 OPC = GENMASK(23, 22);
+	const u32 OPTION = GENMASK(15, 13);
+	const u32 S = BIT(12);
+
+	u32 size = FIELD_GET(SIZE, insn);
+	u32 opc = FIELD_GET(OPC, insn);
+	u32 option = FIELD_GET(OPTION, insn);
+	u32 s = FIELD_GET(S, insn);
+	int scale = size;
+	int extend_len = (option & 0x1) ? 64 : 32;
+	bool extend_unsigned = !(option & 0x4);
+	int shift = s ? scale : 0;
+
+	int n = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RN, insn);
+	int t = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT, insn);
+	int m = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RM, insn);
+	bool is_store;
+	bool is_signed;
+	int regsize;
+	int datasize;
+	u64 offset;
+	u64 address;
+	u64 data;
+
+	if ((opc & 0x2) == 0) {
+		/* store or zero-extending load */
+		is_store = !(opc & 0x1);
+		regsize = size == 0x3 ? 64 : 32;
+		is_signed = false;
+	} else {
+		if (size == 0x3) {
+			if ((opc & 0x1) == 0) {
+				/* prefetch */
+				return 0;
+			}
+			/* undefined */
+			return 1;
+		}
+		/* sign-extending load */
+		is_store = false;
+		if (size == 0x2 && (opc & 0x1) == 0x1) {
+			/* undefined */
+			return 1;
+		}
+		regsize = (opc & 0x1) == 0x1 ? 32 : 64;
+		is_signed = true;
+	}
+
+	datasize = 8 << scale;
+
+	if (n == t && n != 31)
+		return 1;
+
+	offset = pt_regs_read_reg(regs, m);
+	if (extend_len == 32) {
+		offset &= (u32)~0;
+		if (!extend_unsigned)
+			sign_extend64(offset, 31);
+	}
+	offset <<= shift;
+
+	address = regs_get_register(regs, n << 3) + offset;
+
+	if (is_store) {
+		data = pt_regs_read_reg(regs, t);
+		if (align_store(address, datasize / 8, data))
+			return 1;
+	} else {
+		if (align_load(address, datasize / 8, &data))
+			return 1;
+		if (is_signed) {
+			if (regsize == 32)
+				data = sign_extend32(data, datasize - 1);
+			else
+				data = sign_extend64(data, datasize - 1);
+		}
+	}
+
+	return 0;
+}
+
+static int align_ldst_regoff_simdfp(u32 insn, struct pt_regs *regs)
+{
+	const u32 SIZE = GENMASK(31, 30);
+	const u32 OPC = GENMASK(23, 22);
+	const u32 OPTION = GENMASK(15, 13);
+	const u32 S = BIT(12);
+
+	u32 size = FIELD_GET(SIZE, insn);
+	u32 opc = FIELD_GET(OPC, insn);
+	u32 option = FIELD_GET(OPTION, insn);
+	u32 s = FIELD_GET(S, insn);
+	int scale = (opc & 0x2) << 1 | size;
+	int extend_len = (option & 0x1) ? 64 : 32;
+	bool extend_unsigned = !(option & 0x4);
+	int shift = s ? scale : 0;
+
+	int n = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RN, insn);
+	int t = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT, insn);
+	int m = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RM, insn);
+	bool is_store = !(opc & BIT(0));
+	int datasize;
+	u64 offset;
+	u64 address;
+	u64 data_d0, data_d1;
+
+	if ((option & 0x2) == 0)
+		return 1;
+
+	datasize = 8 << scale;
+
+	if (n == t && n != 31)
+		return 1;
+
+	offset = pt_regs_read_reg(regs, m);
+	if (extend_len == 32) {
+		offset &= (u32)~0;
+		if (!extend_unsigned)
+			sign_extend64(offset, 31);
+	}
+	offset <<= shift;
+
+	address = regs_get_register(regs, n << 3) + offset;
+
+	if (is_store) {
+		data_d0 = get_vn_dt(t, 0);
+		if (datasize == 128) {
+			data_d1 = get_vn_dt(t, 1);
+			if (align_store(address, 8, data_d0) ||
+			    align_store(address + 8, 8, data_d1))
+				return 1;
+		} else {
+			if (align_store(address, datasize / 8, data_d0))
+				return 1;
+		}
+	} else {
+		if (datasize == 128) {
+			if (align_load(address, 8, &data_d0) ||
+			    align_load(address + 8, 8, &data_d1))
+				return 1;
+		} else {
+			if (align_load(address, datasize / 8, &data_d0))
+				return 1;
+			data_d1 = 0;
+		}
+		set_vn_dt(t, 0, data_d0);
+		set_vn_dt(t, 1, data_d1);
+	}
+
+	return 0;
+}
+
+static int align_ldst_imm(u32 insn, struct pt_regs *regs)
+{
+	const u32 SIZE = GENMASK(31, 30);
+	const u32 OPC = GENMASK(23, 22);
+
+	u32 size = FIELD_GET(SIZE, insn);
+	u32 opc = FIELD_GET(OPC, insn);
+	bool wback = !(insn & BIT(24)) && !!(insn & BIT(10));
+	bool postindex = wback && !(insn & BIT(11));
+	int scale = size;
+	u64 offset;
+
+	int n = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RN, insn);
+	int t = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT, insn);
+	bool is_store;
+	bool is_signed;
+	int regsize;
+	int datasize;
+	u64 address;
+	u64 data;
+
+	if (!(insn & BIT(24))) {
+		u64 uoffset =
+			aarch64_insn_decode_immediate(AARCH64_INSN_IMM_9, insn);
+		offset = sign_extend64(uoffset, 8);
+	} else {
+		offset = aarch64_insn_decode_immediate(AARCH64_INSN_IMM_12, insn);
+		offset <<= scale;
+	}
+
+	if ((opc & 0x2) == 0) {
+		/* store or zero-extending load */
+		is_store = !(opc & 0x1);
+		regsize = size == 0x3 ? 64 : 32;
+		is_signed = false;
+	} else {
+		if (size == 0x3) {
+			if (FIELD_GET(GENMASK(11, 10), insn) == 0 && (opc & 0x1) == 0) {
+				/* prefetch */
+				return 0;
+			}
+			/* undefined */
+			return 1;
+		}
+		/* sign-extending load */
+		is_store = false;
+		if (size == 0x2 && (opc & 0x1) == 0x1) {
+			/* undefined */
+			return 1;
+		}
+		regsize = (opc & 0x1) == 0x1 ? 32 : 64;
+		is_signed = true;
+	}
+
+	datasize = 8 << scale;
+
+	if (n == t && n != 31)
+		return 1;
+
+	address = regs_get_register(regs, n << 3);
+
+	if (!postindex)
+		address += offset;
+
+	if (is_store) {
+		data = pt_regs_read_reg(regs, t);
+		if (align_store(address, datasize / 8, data))
+			return 1;
+	} else {
+		if (align_load(address, datasize / 8, &data))
+			return 1;
+		if (is_signed) {
+			if (regsize == 32)
+				data = sign_extend32(data, datasize - 1);
+			else
+				data = sign_extend64(data, datasize - 1);
+		}
+		pt_regs_write_reg(regs, t, data);
+	}
+
+	if (wback) {
+		if (postindex)
+			address += offset;
+		if (n == 31)
+			regs->sp = address;
+		else
+			pt_regs_write_reg(regs, n, address);
+	}
+
+	return 0;
+}
+
+static int align_ldst_imm_simdfp(u32 insn, struct pt_regs *regs)
+{
+	const u32 SIZE = GENMASK(31, 30);
+	const u32 OPC = GENMASK(23, 22);
+
+	u32 size = FIELD_GET(SIZE, insn);
+	u32 opc = FIELD_GET(OPC, insn);
+	bool wback = !(insn & BIT(24)) && !!(insn & BIT(10));
+	bool postindex = wback && !(insn & BIT(11));
+	int scale = (opc & 0x2) << 1 | size;
+	u64 offset;
+
+	int n = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RN, insn);
+	int t = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT, insn);
+	bool is_store = !(opc & BIT(0));
+	int datasize;
+	u64 address;
+	u64 data_d0, data_d1;
+
+	if (scale > 4)
+		return 1;
+
+	if (!(insn & BIT(24))) {
+		u64 uoffset =
+			aarch64_insn_decode_immediate(AARCH64_INSN_IMM_9, insn);
+		offset = sign_extend64(uoffset, 8);
+	} else {
+		offset = aarch64_insn_decode_immediate(AARCH64_INSN_IMM_12, insn);
+		offset <<= scale;
+	}
+
+	datasize = 8 << scale;
+
+	address = regs_get_register(regs, n << 3);
+
+	if (!postindex)
+		address += offset;
+
+	if (is_store) {
+		data_d0 = get_vn_dt(t, 0);
+		if (datasize == 128) {
+			data_d1 = get_vn_dt(t, 1);
+			if (align_store(address, 8, data_d0) ||
+			    align_store(address + 8, 8, data_d1))
+				return 1;
+		} else {
+			if (align_store(address, datasize / 8, data_d0))
+				return 1;
+		}
+	} else {
+		if (datasize == 128) {
+			if (align_load(address, 8, &data_d0) ||
+			    align_load(address + 8, 8, &data_d1))
+				return 1;
+		} else {
+			if (align_load(address, datasize / 8, &data_d0))
+				return 1;
+			data_d1 = 0;
+		}
+		set_vn_dt(t, 0, data_d0);
+		set_vn_dt(t, 1, data_d1);
+	}
+
+	if (wback) {
+		if (postindex)
+			address += offset;
+		if (n == 31)
+			regs->sp = address;
+		else
+			pt_regs_write_reg(regs, n, address);
+	}
+
+	return 0;
+}
+
+static int align_ldst_vector_multiple(u32 insn, struct pt_regs *regs)
+{
+	const u32 Q_MASK = BIT(30);
+	const u32 L_MASK = BIT(22);
+	const u32 OPCODE = GENMASK(15, 12);
+	const u32 SIZE = GENMASK(11, 10);
+
+	u32 Q = FIELD_GET(Q_MASK, insn);
+	u32 L = FIELD_GET(L_MASK, insn);
+	u32 opcode = FIELD_GET(OPCODE, insn);
+	u32 size = FIELD_GET(SIZE, insn);
+
+	int t = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT, insn);
+	int n = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RN, insn);
+	int m = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RM, insn);
+	bool wback = !!(insn & BIT(23));
+
+	int datasize = Q ? 128 : 64;
+	int esize = 8 << size;
+	int elements = datasize / esize;
+	int rpt;
+	int selem;
+	u64 address;
+	u64 offs;
+	u64 rval_d0, rval_d1;
+	int tt;
+	int ebytes;
+	int r;
+	int e;
+	int s;
+	u64 data;
+
+	switch (opcode) {
+	case 0: // LD/ST4 (4 registers)
+		rpt = 1;
+		selem = 4;
+		break;
+	case 2: // LD/ST1 (4 registers)
+		rpt = 4;
+		selem = 1;
+		break;
+	case 4: // LD/ST3 (3 registers)
+		rpt = 1;
+		selem = 3;
+		break;
+	case 6: // LD/ST1 (3 registers)
+		rpt = 3;
+		selem = 1;
+		break;
+	case 7: // LD/ST1 (1 register)
+		rpt = 1;
+		selem = 1;
+		break;
+	case 8: // LD/ST2 (2 registers)
+		rpt = 1;
+		selem = 2;
+		break;
+	case 10: // LD/ST1 (2 registers)
+		rpt = 2;
+		selem = 1;
+		break;
+	default:
+		return 1;
+	}
+
+	if (size == 3 && Q == 0 && selem != 1)
+		return 1;
+
+	ebytes = esize / 8;
+
+	address = regs_get_register(regs, n << 3);
+
+	offs = 0;
+
+	for (r = 0; r < rpt; r++) {
+		for (e = 0; e < elements; e++) {
+			tt = (t + r) % 32;
+			for (s = 0; s < selem; s++) {
+				rval_d0 = get_vn_dt(tt, 0);
+				rval_d1 = get_vn_dt(tt, 1);
+				if (L) {
+					if (align_load(address + offs, ebytes, &data))
+						return 1;
+					elem_set(&rval_d1, &rval_d0, e, esize, data);
+					set_vn_dt(tt, 0, rval_d0);
+					set_vn_dt(tt, 1, rval_d1);
+				} else {
+					data = elem_get(rval_d1, rval_d0, e, esize);
+					if (align_store(address + offs, ebytes, data))
+						return 1;
+				}
+				offs += ebytes;
+				tt = (tt + 1) % 32;
+			}
+		}
+	}
+
+	if (wback) {
+		if (m != 31)
+			offs = regs_get_register(regs, m << 3);
+		if (n == 31)
+			regs->sp = address + offs;
+		else
+			pt_regs_write_reg(regs, n, address + offs);
+	}
+
+	return 0;
+}
+
+static int align_ldst_vector_single(u32 insn, struct pt_regs *regs)
+{
+	const u32 Q_MASK = BIT(30);
+	const u32 L_MASK = BIT(22);
+	const u32 R_MASK = BIT(21);
+	const u32 OPCODE = GENMASK(15, 13);
+	const u32 S_MASK = BIT(12);
+	const u32 SIZE = GENMASK(11, 10);
+
+	u32 Q = FIELD_GET(Q_MASK, insn);
+	u32 L = FIELD_GET(L_MASK, insn);
+	u32 R = FIELD_GET(R_MASK, insn);
+	u32 opcode = FIELD_GET(OPCODE, insn);
+	u32 S = FIELD_GET(S_MASK, insn);
+	u32 size = FIELD_GET(SIZE, insn);
+
+	int t = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RT, insn);
+	int n = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RN, insn);
+	int m = aarch64_insn_decode_register(AARCH64_INSN_REGTYPE_RM, insn);
+	bool wback = !!(insn & BIT(23));
+
+	int init_scale = opcode >> 1;
+	int scale = init_scale;
+	int selem = (((opcode & 1) << 1) | R) + 1;
+	bool replicate = false;
+	int index;
+	int datasize;
+	int esize;
+	u64 address;
+	u64 offs;
+	u64 rval_d0, rval_d1;
+	u64 element;
+	int ebytes;
+	int s;
+	u64 data;
+
+	switch (scale) {
+	case 3:
+		if (!L || S)
+			return 1;
+		scale = size;
+		replicate = true;
+		break;
+	case 0:
+		index = (Q << 3) | (S << 2) | size;
+		break;
+	case 1:
+		if (size & 1)
+			return 1;
+		index = (Q << 2) | (S << 1) | (size >> 1);
+		break;
+	case 2:
+		if (size & 2)
+			return 1;
+		if (!(size & 1))
+			index = (Q << 1) | S;
+		else {
+			if (S)
+				return 1;
+			index = Q;
+			scale = 3;
+		}
+		break;
+	}
+
+	datasize = Q ? 128 : 64;
+	esize = 8 << scale;
+
+	ebytes = esize / 8;
+
+	address = regs_get_register(regs, n << 3);
+
+	offs = 0;
+
+	if (replicate) {
+		for (s = 0; s < selem; s++) {
+			if (align_load(address + offs, ebytes, &element))
+				return 1;
+			data = replicate64(element, esize);
+			set_vn_dt(t, 0, data);
+			if (datasize == 128)
+				set_vn_dt(t, 1, data);
+			else
+				set_vn_dt(t, 1, 0);
+			offs += ebytes;
+			t = (t + 1) & 31;
+		}
+	} else {
+		for (s = 0; s < selem; s++) {
+			rval_d0 = get_vn_dt(t, 0);
+			rval_d1 = get_vn_dt(t, 1);
+			if (L) {
+				if (align_load(address + offs, ebytes, &data))
+					return 1;
+				elem_set(&rval_d1, &rval_d0, index, esize, data);
+				set_vn_dt(t, 0, rval_d0);
+				set_vn_dt(t, 1, rval_d1);
+			} else {
+				data = elem_get(rval_d1, rval_d0, index, esize);
+				if (align_store(address + offs, ebytes, data))
+					return 1;
+			}
+			offs += ebytes;
+			t = (t + 1) & 31;
+		}
+	}
+
+	if (wback) {
+		if (m != 31)
+			offs = regs_get_register(regs, m << 3);
+		if (n == 31)
+			regs->sp = address + offs;
+		else
+			pt_regs_write_reg(regs, n, address + offs);
+	}
+
+	return 0;
+}
+
+static int align_ldst(u32 insn, struct pt_regs *regs)
+{
+	const u32 op0 = FIELD_GET(GENMASK(31, 28), insn);
+	const u32 op1 = FIELD_GET(BIT(26), insn);
+	const u32 op2 = FIELD_GET(GENMASK(24, 23), insn);
+	const u32 op3 = FIELD_GET(GENMASK(21, 16), insn);
+	const u32 op4 = FIELD_GET(GENMASK(11, 10), insn);
+
+	if ((op0 & 0x3) == 0x2) {
+		/*
+		 * |------+-----+-----+-----+-----+-----------------------------------------|
+		 * | op0  | op1 | op2 | op3 | op4 | Decode group                            |
+		 * |------+-----+-----+-----+-----+-----------------------------------------|
+		 * | xx10 | -   |  00 | -   | -   | Load/store no-allocate pair (offset)    |
+		 * | xx10 | -   |  01 | -   | -   | Load/store register pair (post-indexed) |
+		 * | xx10 | -   |  10 | -   | -   | Load/store register pair (offset)       |
+		 * | xx10 | -   |  11 | -   | -   | Load/store register pair (pre-indexed)  |
+		 * |------+-----+-----+-----+-----+-----------------------------------------|
+		 */
+
+		if (op1 == 0) { /* V == 0 */
+			/* general */
+			return align_ldst_pair(insn, regs);
+		}
+		/* simdfp */
+		return align_ldst_pair_simdfp(insn, regs);
+	} else if ((op0 & 0x3) == 0x3 &&
+		   (((op2 & 0x2) == 0 && (op3 & 0x20) == 0 && op4 != 0x2) ||
+		    ((op2 & 0x2) == 0x2))) {
+		/*
+		 * |------+-----+-----+--------+-----+---------------------------------------------|
+		 * | op0  | op1 | op2 |    op3 | op4 | Decode group                                |
+		 * |------+-----+-----+--------+-----+---------------------------------------------|
+		 * | xx11 | -   |  0x | 0xxxxx |  00 | Load/store register (unscaled immediate)    |
+		 * | xx11 | -   |  0x | 0xxxxx |  01 | Load/store register (immediate post-indexed |
+		 * | xx11 | -   |  0x | 0xxxxx |  11 | Load/store register (immediate pre-indexed) |
+		 * | xx11 | -   |  1x |      - |   - | Load/store register (unsigned immediate)    |
+		 * |------+-----+-----+--------+-----+---------------------------------------------|
+		 */
+
+		if (op1 == 0) {  /* V == 0 */
+			/* general */
+			return align_ldst_imm(insn, regs);
+		}
+		/* simdfp */
+		return align_ldst_imm_simdfp(insn, regs);
+	} else if ((op0 & 0x3) == 0x3 && (op2 & 0x2) == 0 &&
+		   (op3 & 0x20) == 0x20 && op4 == 0x2) {
+		/*
+		 * |------+-----+-----+--------+-----+---------------------------------------|
+		 * | op0  | op1 | op2 |    op3 | op4 |                                       |
+		 * |------+-----+-----+--------+-----+---------------------------------------|
+		 * | xx11 | -   |  0x | 1xxxxx |  10 | Load/store register (register offset) |
+		 * |------+-----+-----+--------+-----+---------------------------------------|
+		 */
+		if (op1 == 0) { /* V == 0 */
+			/* general */
+			return align_ldst_regoff(insn, regs);
+		}
+		/* simdfp */
+		return align_ldst_regoff_simdfp(insn, regs);
+	} else if ((op0 & 0xb) == 0 && op1 == 1 &&
+		   ((op2 == 0 && op3 == 0) || (op2 == 1 && ((op3 & 0x20) == 0)))) {
+		/*
+		 * |------+-----+-----+--------+-----+---------------------------------------------|
+		 * | op0  | op1 | op2 |    op3 | op4 |                                             |
+		 * |------+-----+-----+--------+-----+---------------------------------------------|
+		 * | 0x00 |   1 |  00 | 000000 |   - | Advanced SIMD load/store multiple structure |
+		 * | 0x00 |   1 |  01 | 0xxxxx |   - | Advanced SIMD load/store multiple structure |
+		 * |      |     |     |        |     |   (post-indexed)                            |
+		 * |------+-----+-----+--------+-----+---------------------------------------------|
+		 */
+		return align_ldst_vector_multiple(insn, regs);
+	} else if ((op0 & 0xb) == 0 && op1 == 1 &&
+		   ((op2 == 2 && ((op3 & 0x1f) == 0)) || op2 == 3)) {
+		/*
+		 * |------+-----+-----+--------+-----+-------------------------------------------|
+		 * | op0  | op1 | op2 |    op3 | op4 |                                           |
+		 * |------+-----+-----+--------+-----+-------------------------------------------|
+		 * | 0x00 |   1 |  10 | x00000 |   - | Advanced SIMD load/store single structure |
+		 * | 0x00 |   1 |  11 |      - |   - | Advanced SIMD load/store single structure |
+		 * |      |     |     |        |     |   (post-indexed)                          |
+		 * |------+-----+-----+--------+-----+-------------------------------------------|
+		 */
+		return align_ldst_vector_single(insn, regs);
+	} else
+		return 1;
+}
+
+int do_alignment_fixup(unsigned long addr, unsigned int esr,
+		       struct pt_regs *regs)
+{
+	u32 insn;
+	int res;
+
+	if (user_mode(regs)) {
+		__le32 insn_le;
+
+		if (!is_ttbr0_addr(addr))
+			return 1;
+
+		if (get_user(insn_le,
+			     (__le32 __user *)instruction_pointer(regs)))
+			return 1;
+		insn = le32_to_cpu(insn_le);
+	} else {
+		if (aarch64_insn_read((void *)instruction_pointer(regs), &insn))
+			return 1;
+	}
+
+	if (__aarch64_insn_is_class_ldst(insn))
+		res = align_ldst(insn, regs);
+	else if (__aarch64_insn_is_dc_zva(insn))
+		res = align_dc_zva(addr, regs);
+	else
+		res = 1;
+
+	if (!res) {
+		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, regs->pc);
+		arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	}
+	return res;
+}
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 55f6455a82843..e43e5b1c33f19 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -718,8 +718,9 @@ static int __kprobes do_translation_fault(unsigned long far,
 static int do_alignment_fault(unsigned long far, unsigned long esr,
 			      struct pt_regs *regs)
 {
-	if (IS_ENABLED(CONFIG_COMPAT_ALIGNMENT_FIXUPS) &&
-	    compat_user_mode(regs))
+	if (!compat_user_mode(regs))
+		return do_alignment_fixup(far, esr, regs);
+	else if (IS_ENABLED(CONFIG_COMPAT_ALIGNMENT_FIXUPS))
 		return do_compat_alignment_fixup(far, regs);
 	do_bad_area(far, esr, regs);
 	return 0;
-- 
2.43.0

